{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_for_summary = 0 if torch.cuda.is_available() else -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"vinai/bartpho-syllable\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device_for_summary\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói t·∫£i m√¥ h√¨nh BARTpho-syllable: {e}\")\n",
    "    summarizer = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(full_text):\n",
    "    \"\"\"\n",
    "    T√≥m t·∫Øt vƒÉn b·∫£n s·ª≠ d·ª•ng BartPho-syllable.\n",
    "    \"\"\"\n",
    "    if not summarizer:\n",
    "        return \"M√¥ h√¨nh t√≥m t·∫Øt ch∆∞a s·∫µn s√†ng.\"\n",
    "    if not full_text.strip():\n",
    "        return \"Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt.\"\n",
    "\n",
    "    # G·ªçi pipeline t√≥m t·∫Øt\n",
    "    result = summarizer(\n",
    "        full_text,\n",
    "        max_length=100, \n",
    "        min_length=30,\n",
    "        truncation=True,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=2.0,\n",
    "        length_penalty=1.0,\n",
    "        temperature=0.0,\n",
    "        do_sample=False,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    key = 'summary_text' if 'summary_text' in result[0] else 'generated_text'\n",
    "    return result[0][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) CH·∫†Y TH·ª¨ M√î H√åNH V·ªöI ƒêO·∫†N VƒÇN B·∫¢N M·∫™U\n",
    "################################################################################\n",
    "text = \"\"\"\n",
    "H√¥m nay ch√∫ng ta h·ªçp ƒë·ªÉ b√†n v·ªÅ k·∫ø ho·∫°ch tri·ªÉn khai d·ª± √°n qu√Ω t·ªõi. \n",
    "ƒê·∫ßu ti√™n, nh√≥m k·ªπ thu·∫≠t s·∫Ω c·∫ßn ho√†n th√†nh vi·ªác n√¢ng c·∫•p h·ªá th·ªëng v√†o cu·ªëi th√°ng n√†y. \n",
    "ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ti·∫øn ƒë·ªô chung. \n",
    "Ti·∫øp theo, nh√≥m kinh doanh ƒë√£ li√™n h·ªá v·ªõi m·ªôt s·ªë ƒë·ªëi t√°c v√† h·ªç c·∫ßn ph·∫£n h·ªìi s·ªõm ƒë·ªÉ k·ªãp k√Ω k·∫øt h·ª£p ƒë·ªìng. \n",
    "Ngo√†i ra, v·∫•n ƒë·ªÅ ng√¢n s√°ch c≈©ng c·∫ßn ƒë∆∞·ª£c xem x√©t v√¨ c√≥ m·ªôt s·ªë kho·∫£n chi ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát. \n",
    "Ch√∫ng ta c·∫ßn ƒë∆∞a ra quy·∫øt ƒë·ªãnh s·ªõm ƒë·ªÉ ƒë·∫£m b·∫£o ngu·ªìn l·ª±c ƒë·∫ßy ƒë·ªß. \n",
    "Cu·ªëi c√πng, t√¥i ƒë·ªÅ xu·∫•t t·ªï ch·ª©c m·ªôt cu·ªôc h·ªçp nh·ªè v√†o tu·∫ßn sau ƒë·ªÉ ki·ªÉm tra ti·∫øn ƒë·ªô t·ª´ng nh√≥m tr∆∞·ªõc khi c√≥ b√°o c√°o ch√≠nh th·ª©c. \n",
    "N·∫øu kh√¥ng c√≤n √Ω ki·∫øn g√¨ th√™m, ch√∫ng ta k·∫øt th√∫c cu·ªôc h·ªçp t·∫°i ƒë√¢y.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå VƒÉn b·∫£n g·ªëc:\n",
      "\n",
      "H√¥m nay ch√∫ng ta h·ªçp ƒë·ªÉ b√†n v·ªÅ k·∫ø ho·∫°ch tri·ªÉn khai d·ª± √°n qu√Ω t·ªõi. \n",
      "ƒê·∫ßu ti√™n, nh√≥m k·ªπ thu·∫≠t s·∫Ω c·∫ßn ho√†n th√†nh vi·ªác n√¢ng c·∫•p h·ªá th·ªëng v√†o cu·ªëi th√°ng n√†y. \n",
      "ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ti·∫øn ƒë·ªô chung. \n",
      "Ti·∫øp theo, nh√≥m kinh doanh ƒë√£ li√™n h·ªá v·ªõi m·ªôt s·ªë ƒë·ªëi t√°c v√† h·ªç c·∫ßn ph·∫£n h·ªìi s·ªõm ƒë·ªÉ k·ªãp k√Ω k·∫øt h·ª£p ƒë·ªìng. \n",
      "Ngo√†i ra, v·∫•n ƒë·ªÅ ng√¢n s√°ch c≈©ng c·∫ßn ƒë∆∞·ª£c xem x√©t v√¨ c√≥ m·ªôt s·ªë kho·∫£n chi ch∆∞a ƒë∆∞·ª£c ph√™ duy·ªát. \n",
      "Ch√∫ng ta c·∫ßn ƒë∆∞a ra quy·∫øt ƒë·ªãnh s·ªõm ƒë·ªÉ ƒë·∫£m b·∫£o ngu·ªìn l·ª±c ƒë·∫ßy ƒë·ªß. \n",
      "Cu·ªëi c√πng, t√¥i ƒë·ªÅ xu·∫•t t·ªï ch·ª©c m·ªôt cu·ªôc h·ªçp nh·ªè v√†o tu·∫ßn sau ƒë·ªÉ ki·ªÉm tra ti·∫øn ƒë·ªô t·ª´ng nh√≥m tr∆∞·ªõc khi c√≥ b√°o c√°o ch√≠nh th·ª©c. \n",
      "N·∫øu kh√¥ng c√≤n √Ω ki·∫øn g√¨ th√™m, ch√∫ng ta k·∫øt th√∫c cu·ªôc h·ªçp t·∫°i ƒë√¢y.\n",
      "\n",
      "\n",
      "üìå VƒÉn b·∫£n t√≥m t·∫Øt:\n",
      "H√¥m nay ch√∫ng ta h·ªçp ƒë·ªÉ b√†n v·ªÅ k·∫ø ho·∫°ch tri·ªÉn khai d·ª± √°n qu√Ω t·ªõi. ƒê·∫ßu ti√™n, nh√≥m k·ªπ thu·∫≠t s·∫Ω c·∫ßn ho√†n th√†nh vi·ªác n√¢ng c·∫•p h·ªá th·ªëng v√†o cu·ªëi th√°ng n√†y. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ti·∫øn ƒë·ªô chung. Ti·∫øp theo, nh√≥m kinh doanh ƒë√£ li√™n h·ªá v·ªõi m·ªôt s·ªë ƒë·ªëi t√°c v√† h·ªç c·∫ßn ph·∫£n h·ªìi s·ªõm ƒë·ªÉ k·ªãp k√Ω k·∫øt h·ª£p ƒë·ªìng. Ngo√†i ra, v·∫•n ƒë·ªÅ ng√¢n s√°ch c≈©ng c·∫ßn ƒë∆∞·ª£c xem x√©t v√¨ c√≥ m·ªôt s·ªë kho·∫£n\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(text)\n",
    "print(\"\\nüìå VƒÉn b·∫£n g·ªëc:\")\n",
    "print(text)\n",
    "print(\"\\nüìå VƒÉn b·∫£n t√≥m t·∫Øt:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m app \u001b[38;5;241m=\u001b[39m build_gradio_app()\n\u001b[0;32m      3\u001b[0m app\u001b[38;5;241m.\u001b[39mqueue()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7860\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py:2572\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[1;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[0;32m   2567\u001b[0m     (\n\u001b[0;32m   2568\u001b[0m         server_name,\n\u001b[0;32m   2569\u001b[0m         server_port,\n\u001b[0;32m   2570\u001b[0m         local_url,\n\u001b[0;32m   2571\u001b[0m         server,\n\u001b[1;32m-> 2572\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\http_server.py:156\u001b[0m, in \u001b[0;36mstart_server\u001b[1;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     app = build_gradio_app()\n",
    "#     app.queue()\n",
    "#     app.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVietAI/vit5-large-vietnews-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVietAI/vit5-large-vietnews-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVietAI l√† t·ªï ch·ª©c phi l·ª£i nhu·∫≠n v·ªõi s·ª© m·ªánh ∆∞∆°m m·∫ßm t√†i nƒÉng v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o v√† x√¢y d·ª±ng m·ªôt c·ªông ƒë·ªìng c√°c chuy√™n gia trong lƒ©nh v·ª±c tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫≥ng c·∫•p qu·ªëc t·∫ø t·∫°i Vi·ªát Nam.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvietnews: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m </s>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3070\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3066\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3067\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3068\u001b[0m         )\n\u001b[0;32m   3069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Trung Kien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-large-vietnews-summarization\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-large-vietnews-summarization\")\n",
    "model.cuda()\n",
    "\n",
    "sentence = \"VietAI l√† t·ªï ch·ª©c phi l·ª£i nhu·∫≠n v·ªõi s·ª© m·ªánh ∆∞∆°m m·∫ßm t√†i nƒÉng v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o v√† x√¢y d·ª±ng m·ªôt c·ªông ƒë·ªìng c√°c chuy√™n gia trong lƒ©nh v·ª±c tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫≥ng c·∫•p qu·ªëc t·∫ø t·∫°i Vi·ªát Nam.\"\n",
    "text =  \"vietnews: \" + sentence + \" </s>\"\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    early_stopping=True\n",
    ")\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
